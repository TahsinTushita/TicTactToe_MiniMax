{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\Tushita\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  PRICE  \n",
       "0       15.3  396.90   4.98   24.0  \n",
       "1       17.8  396.90   9.14   21.6  \n",
       "2       17.8  392.83   4.03   34.7  \n",
       "3       18.7  394.63   2.94   33.4  \n",
       "4       18.7  396.90   5.33   36.2  \n",
       "..       ...     ...    ...    ...  \n",
       "501     21.0  391.99   9.67   22.4  \n",
       "502     21.0  396.90   9.08   20.6  \n",
       "503     21.0  396.90   5.64   23.9  \n",
       "504     21.0  393.45   6.48   22.0  \n",
       "505     21.0  396.90   7.88   11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PRICE'] = dataset.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "PRICE      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = df.iloc[:,0:13]\n",
    "df_y = df.iloc[:,-1]\n",
    "df_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: PRICE, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\tushita\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.0-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Processing c:\\users\\tushita\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Processing c:\\users\\tushita\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\\wrapt-1.12.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tushita\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.7-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: absl-py, gast, astunparse, termcolor, opt-einsum, keras-preprocessing, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, tensorboard-plugin-wit, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, wrapt, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 keras-preprocessing-1.1.2 markdown-3.3.3 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                448       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(13,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df_x, df_y, test_size=0.4,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "31/31 [==============================] - 2s 37ms/step - loss: 1720.7485 - val_loss: 130.2254\n",
      "Epoch 2/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 109.7916 - val_loss: 88.4054\n",
      "Epoch 3/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 81.2056 - val_loss: 73.6560\n",
      "Epoch 4/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 57.1346 - val_loss: 64.8721\n",
      "Epoch 5/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 65.0117 - val_loss: 67.4042\n",
      "Epoch 6/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 65.6197 - val_loss: 60.7459\n",
      "Epoch 7/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 61.0631 - val_loss: 58.9743\n",
      "Epoch 8/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 38.0010 - val_loss: 57.8236\n",
      "Epoch 9/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 49.2849 - val_loss: 55.1651\n",
      "Epoch 10/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 39.7800 - val_loss: 57.3475\n",
      "Epoch 11/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 69.5575 - val_loss: 69.7037\n",
      "Epoch 12/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 61.0069 - val_loss: 50.4431\n",
      "Epoch 13/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 35.7226 - val_loss: 47.7440\n",
      "Epoch 14/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 41.8375 - val_loss: 45.3465\n",
      "Epoch 15/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 51.5027 - val_loss: 54.6539\n",
      "Epoch 16/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 36.4890 - val_loss: 42.2231\n",
      "Epoch 17/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 56.9090 - val_loss: 42.0705\n",
      "Epoch 18/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 32.4880 - val_loss: 47.9213\n",
      "Epoch 19/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 47.0634 - val_loss: 44.8747\n",
      "Epoch 20/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 51.2125 - val_loss: 40.7071\n",
      "Epoch 21/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 44.4126 - val_loss: 39.9205\n",
      "Epoch 22/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 31.9333 - val_loss: 37.2576\n",
      "Epoch 23/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 23.0905 - val_loss: 36.8074\n",
      "Epoch 24/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 35.8118 - val_loss: 43.6955\n",
      "Epoch 25/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 31.3515 - val_loss: 39.8169\n",
      "Epoch 26/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 31.9710 - val_loss: 48.4077\n",
      "Epoch 27/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 35.9086 - val_loss: 40.8500\n",
      "Epoch 28/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 38.4470 - val_loss: 45.8301\n",
      "Epoch 29/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 24.2718 - val_loss: 38.8877\n",
      "Epoch 30/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 34.2621 - val_loss: 36.3069\n",
      "Epoch 31/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 35.6255 - val_loss: 32.8495\n",
      "Epoch 32/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 36.2551 - val_loss: 31.2130\n",
      "Epoch 33/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 27.5504 - val_loss: 36.6746\n",
      "Epoch 34/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 27.5268 - val_loss: 33.4654\n",
      "Epoch 35/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 29.5137 - val_loss: 31.7206\n",
      "Epoch 36/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 30.0106 - val_loss: 34.0478\n",
      "Epoch 37/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 34.2090 - val_loss: 31.4394\n",
      "Epoch 38/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 30.1426 - val_loss: 35.4391\n",
      "Epoch 39/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 27.8356 - val_loss: 33.8743\n",
      "Epoch 40/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 27.5411 - val_loss: 31.6026\n",
      "Epoch 41/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 32.9107 - val_loss: 30.9803\n",
      "Epoch 42/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 28.2192 - val_loss: 41.4814\n",
      "Epoch 43/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 32.8782 - val_loss: 36.1261\n",
      "Epoch 44/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.9766 - val_loss: 34.3582\n",
      "Epoch 45/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 28.9207 - val_loss: 28.4689\n",
      "Epoch 46/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 28.7878 - val_loss: 32.2283\n",
      "Epoch 47/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 26.8659 - val_loss: 29.6518\n",
      "Epoch 48/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 35.5413 - val_loss: 29.1075\n",
      "Epoch 49/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 26.9574 - val_loss: 30.8673\n",
      "Epoch 50/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 24.4897 - val_loss: 32.4226\n",
      "Epoch 51/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 33.7732 - val_loss: 34.0333\n",
      "Epoch 52/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 26.1115 - val_loss: 40.3108\n",
      "Epoch 53/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 31.4394 - val_loss: 29.8931\n",
      "Epoch 54/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 29.4226 - val_loss: 34.8557\n",
      "Epoch 55/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 26.5914 - val_loss: 28.3256\n",
      "Epoch 56/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 32.2475 - val_loss: 31.3650\n",
      "Epoch 57/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 27.9775 - val_loss: 28.5166\n",
      "Epoch 58/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 21.6322 - val_loss: 30.1992\n",
      "Epoch 59/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 30.5872 - val_loss: 28.0419\n",
      "Epoch 60/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.1316 - val_loss: 30.7918\n",
      "Epoch 61/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.7171 - val_loss: 30.2800\n",
      "Epoch 62/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 23.8708 - val_loss: 28.4564\n",
      "Epoch 63/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.6311 - val_loss: 28.1917\n",
      "Epoch 64/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 24.3830 - val_loss: 30.4350\n",
      "Epoch 65/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 23.6893 - val_loss: 28.1084\n",
      "Epoch 66/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 19.2948 - val_loss: 32.5459\n",
      "Epoch 67/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.5007 - val_loss: 29.0898\n",
      "Epoch 68/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 21.9076 - val_loss: 29.4118\n",
      "Epoch 69/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 20.6490 - val_loss: 35.7780\n",
      "Epoch 70/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 21.2627 - val_loss: 33.9752\n",
      "Epoch 71/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 23.5451 - val_loss: 35.0007\n",
      "Epoch 72/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 20.2489 - val_loss: 33.7891\n",
      "Epoch 73/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.1592 - val_loss: 29.3081\n",
      "Epoch 74/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 25.2516 - val_loss: 28.6640\n",
      "Epoch 75/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 19.5237 - val_loss: 27.8729\n",
      "Epoch 76/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.3770 - val_loss: 39.5202\n",
      "Epoch 77/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 27.2752 - val_loss: 38.1959\n",
      "Epoch 78/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.2778 - val_loss: 34.9860\n",
      "Epoch 79/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 26.2202 - val_loss: 34.2957\n",
      "Epoch 80/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 25.7962 - val_loss: 30.6284\n",
      "Epoch 81/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 4ms/step - loss: 23.8405 - val_loss: 28.0632\n",
      "Epoch 82/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.7857 - val_loss: 26.5065\n",
      "Epoch 83/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.7060 - val_loss: 26.8645\n",
      "Epoch 84/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 27.9497 - val_loss: 26.7761\n",
      "Epoch 85/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 22.7523 - val_loss: 26.9046\n",
      "Epoch 86/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 24.7075 - val_loss: 30.4967\n",
      "Epoch 87/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.7636 - val_loss: 28.0678\n",
      "Epoch 88/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 22.2390 - val_loss: 30.5130\n",
      "Epoch 89/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 25.9553 - val_loss: 28.9782\n",
      "Epoch 90/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 22.8989 - val_loss: 28.5279\n",
      "Epoch 91/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.4676 - val_loss: 30.5879\n",
      "Epoch 92/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 23.3809 - val_loss: 28.7471\n",
      "Epoch 93/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 17.8633 - val_loss: 25.9398\n",
      "Epoch 94/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 20.6070 - val_loss: 30.9059\n",
      "Epoch 95/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 18.1946 - val_loss: 27.4173\n",
      "Epoch 96/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 18.1489 - val_loss: 25.6788\n",
      "Epoch 97/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 20.0151 - val_loss: 27.2623\n",
      "Epoch 98/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 20.7197 - val_loss: 26.0175\n",
      "Epoch 99/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.4650 - val_loss: 27.1993\n",
      "Epoch 100/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 21.9426 - val_loss: 30.9215\n",
      "Epoch 101/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 26.1664 - val_loss: 25.8618\n",
      "Epoch 102/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 16.5816 - val_loss: 26.1298\n",
      "Epoch 103/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.6033 - val_loss: 34.3722\n",
      "Epoch 104/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 26.5815 - val_loss: 25.1438\n",
      "Epoch 105/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 19.3047 - val_loss: 26.0046\n",
      "Epoch 106/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.0030 - val_loss: 28.2022\n",
      "Epoch 107/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.0770 - val_loss: 41.5702\n",
      "Epoch 108/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 21.2454 - val_loss: 33.4576\n",
      "Epoch 109/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.4195 - val_loss: 28.1767\n",
      "Epoch 110/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.3108 - val_loss: 28.6860\n",
      "Epoch 111/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.2007 - val_loss: 27.8443\n",
      "Epoch 112/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 21.0153 - val_loss: 25.2721\n",
      "Epoch 113/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.8081 - val_loss: 24.2706\n",
      "Epoch 114/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.9840 - val_loss: 30.6005\n",
      "Epoch 115/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 21.2538 - val_loss: 29.6815\n",
      "Epoch 116/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.6942 - val_loss: 25.9277\n",
      "Epoch 117/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.5122 - val_loss: 26.3615\n",
      "Epoch 118/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.0151 - val_loss: 25.8905\n",
      "Epoch 119/400\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 18.9292 - val_loss: 26.6220\n",
      "Epoch 120/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.8475 - val_loss: 29.0426\n",
      "Epoch 121/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.4766 - val_loss: 33.0982\n",
      "Epoch 122/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.5513 - val_loss: 26.4101\n",
      "Epoch 123/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.3217 - val_loss: 33.1301\n",
      "Epoch 124/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.2596 - val_loss: 25.8378\n",
      "Epoch 125/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.6983 - val_loss: 25.3474\n",
      "Epoch 126/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.1618 - val_loss: 25.0181\n",
      "Epoch 127/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.6028 - val_loss: 25.4350\n",
      "Epoch 128/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.6428 - val_loss: 25.0650\n",
      "Epoch 129/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.8432 - val_loss: 26.0930\n",
      "Epoch 130/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.2026 - val_loss: 33.8191\n",
      "Epoch 131/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.3098 - val_loss: 30.4673\n",
      "Epoch 132/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.6567 - val_loss: 29.3222\n",
      "Epoch 133/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.4764 - val_loss: 25.5679\n",
      "Epoch 134/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 21.3411 - val_loss: 26.8166\n",
      "Epoch 135/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.9122 - val_loss: 23.8486\n",
      "Epoch 136/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.1153 - val_loss: 23.5163\n",
      "Epoch 137/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.9157 - val_loss: 24.1272\n",
      "Epoch 138/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.2277 - val_loss: 27.9693\n",
      "Epoch 139/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.1647 - val_loss: 26.1444\n",
      "Epoch 140/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.9210 - val_loss: 25.9581\n",
      "Epoch 141/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.7049 - val_loss: 25.3202\n",
      "Epoch 142/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.5967 - val_loss: 27.2777\n",
      "Epoch 143/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.1674 - val_loss: 24.9236\n",
      "Epoch 144/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.5589 - val_loss: 25.9362\n",
      "Epoch 145/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.0963 - val_loss: 26.6060\n",
      "Epoch 146/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.3490 - val_loss: 30.2614\n",
      "Epoch 147/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.0274 - val_loss: 23.9654\n",
      "Epoch 148/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 21.8862 - val_loss: 24.4822\n",
      "Epoch 149/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.7542 - val_loss: 25.6228\n",
      "Epoch 150/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.3227 - val_loss: 54.3931\n",
      "Epoch 151/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.5579 - val_loss: 38.5514\n",
      "Epoch 152/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 27.5816 - val_loss: 26.0330\n",
      "Epoch 153/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.9911 - val_loss: 32.1078\n",
      "Epoch 154/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.2938 - val_loss: 25.5705\n",
      "Epoch 155/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.6944 - val_loss: 34.1735\n",
      "Epoch 156/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.8955 - val_loss: 23.0254\n",
      "Epoch 157/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.9419 - val_loss: 27.4153\n",
      "Epoch 158/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.5361 - val_loss: 26.9653\n",
      "Epoch 159/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.7185 - val_loss: 28.6284\n",
      "Epoch 160/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.8914 - val_loss: 32.2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 22.7128 - val_loss: 27.6323\n",
      "Epoch 162/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.2553 - val_loss: 28.6734\n",
      "Epoch 163/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 19.2786 - val_loss: 26.4261\n",
      "Epoch 164/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 14.9091 - val_loss: 25.6540\n",
      "Epoch 165/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.3610 - val_loss: 29.0233\n",
      "Epoch 166/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.2196 - val_loss: 32.5329\n",
      "Epoch 167/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 18.4953 - val_loss: 33.5755\n",
      "Epoch 168/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.9923 - val_loss: 23.9861\n",
      "Epoch 169/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.6247 - val_loss: 22.8676\n",
      "Epoch 170/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.5489 - val_loss: 29.0824\n",
      "Epoch 171/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.3422 - val_loss: 26.2906\n",
      "Epoch 172/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.2770 - val_loss: 24.4177\n",
      "Epoch 173/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.7083 - val_loss: 23.7590\n",
      "Epoch 174/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.4291 - val_loss: 24.6356\n",
      "Epoch 175/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.4877 - val_loss: 25.6003\n",
      "Epoch 176/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.3068 - val_loss: 24.7013\n",
      "Epoch 177/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 14.6199 - val_loss: 30.4246\n",
      "Epoch 178/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 18.4200 - val_loss: 25.2138\n",
      "Epoch 179/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.7962 - val_loss: 25.1523\n",
      "Epoch 180/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.9854 - val_loss: 26.9122\n",
      "Epoch 181/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 14.8957 - val_loss: 26.9292\n",
      "Epoch 182/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.8995 - val_loss: 24.1509\n",
      "Epoch 183/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.5484 - val_loss: 27.1603\n",
      "Epoch 184/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.9852 - val_loss: 33.1312\n",
      "Epoch 185/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 22.0957 - val_loss: 28.0033\n",
      "Epoch 186/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.8802 - val_loss: 26.2798\n",
      "Epoch 187/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.6834 - val_loss: 26.3851\n",
      "Epoch 188/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.8825 - val_loss: 25.0563\n",
      "Epoch 189/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.0288 - val_loss: 25.7247\n",
      "Epoch 190/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 16.6885 - val_loss: 32.5600\n",
      "Epoch 191/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.9806 - val_loss: 24.9990\n",
      "Epoch 192/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 16.3901 - val_loss: 30.8448\n",
      "Epoch 193/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 17.5740 - val_loss: 25.6367\n",
      "Epoch 194/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.3939 - val_loss: 24.2931\n",
      "Epoch 195/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.2042 - val_loss: 25.6617\n",
      "Epoch 196/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.9232 - val_loss: 26.4694\n",
      "Epoch 197/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 13.9998 - val_loss: 26.0999\n",
      "Epoch 198/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.4417 - val_loss: 24.6217\n",
      "Epoch 199/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 15.5728 - val_loss: 25.8521\n",
      "Epoch 200/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.8075 - val_loss: 21.8750\n",
      "Epoch 201/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.2907 - val_loss: 26.0720\n",
      "Epoch 202/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 23.1159 - val_loss: 37.5097\n",
      "Epoch 203/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 20.5769 - val_loss: 23.2374\n",
      "Epoch 204/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 13.8505 - val_loss: 25.0024\n",
      "Epoch 205/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.0871 - val_loss: 23.3351\n",
      "Epoch 206/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 16.6682 - val_loss: 24.0617\n",
      "Epoch 207/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.1480 - val_loss: 21.9543\n",
      "Epoch 208/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 13.8516 - val_loss: 23.3621\n",
      "Epoch 209/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.2545 - val_loss: 24.1747\n",
      "Epoch 210/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 16.3331 - val_loss: 23.4133\n",
      "Epoch 211/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.0311 - val_loss: 36.0168\n",
      "Epoch 212/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.1098 - val_loss: 25.7410\n",
      "Epoch 213/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.0266 - val_loss: 23.3158\n",
      "Epoch 214/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.4451 - val_loss: 34.7360\n",
      "Epoch 215/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.1166 - val_loss: 40.2528\n",
      "Epoch 216/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 20.1393 - val_loss: 26.5016\n",
      "Epoch 217/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.0325 - val_loss: 24.5413\n",
      "Epoch 218/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 17.8572 - val_loss: 31.7602\n",
      "Epoch 219/400\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 13.7533 - val_loss: 23.0743\n",
      "Epoch 220/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.5448 - val_loss: 23.0752\n",
      "Epoch 221/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.2179 - val_loss: 23.7618\n",
      "Epoch 222/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.4926 - val_loss: 33.5110\n",
      "Epoch 223/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.7419 - val_loss: 23.9356\n",
      "Epoch 224/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.6030 - val_loss: 29.2381\n",
      "Epoch 225/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.7861 - val_loss: 24.2831\n",
      "Epoch 226/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.4833 - val_loss: 23.4813\n",
      "Epoch 227/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.0921 - val_loss: 21.7279\n",
      "Epoch 228/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.5359 - val_loss: 23.9923\n",
      "Epoch 229/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.9877 - val_loss: 35.8714\n",
      "Epoch 230/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 19.1063 - val_loss: 26.4474\n",
      "Epoch 231/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.2169 - val_loss: 24.6011\n",
      "Epoch 232/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.0687 - val_loss: 22.8016\n",
      "Epoch 233/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.2002 - val_loss: 22.9271\n",
      "Epoch 234/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.7110 - val_loss: 27.2321\n",
      "Epoch 235/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.9329 - val_loss: 22.4689\n",
      "Epoch 236/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.4612 - val_loss: 32.1951\n",
      "Epoch 237/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.5806 - val_loss: 25.4445\n",
      "Epoch 238/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.6393 - val_loss: 26.6748\n",
      "Epoch 239/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 18.2549 - val_loss: 25.6802\n",
      "Epoch 240/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 3ms/step - loss: 19.4400 - val_loss: 24.3940\n",
      "Epoch 241/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.5544 - val_loss: 39.0968\n",
      "Epoch 242/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 23.5912 - val_loss: 43.2140\n",
      "Epoch 243/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 24.4652 - val_loss: 24.7961\n",
      "Epoch 244/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.5154 - val_loss: 33.0474\n",
      "Epoch 245/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.9050 - val_loss: 26.0260\n",
      "Epoch 246/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.5948 - val_loss: 23.9913\n",
      "Epoch 247/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.3841 - val_loss: 24.7868\n",
      "Epoch 248/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.3956 - val_loss: 26.8452\n",
      "Epoch 249/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.8661 - val_loss: 22.8721\n",
      "Epoch 250/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.5504 - val_loss: 31.0339\n",
      "Epoch 251/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.3897 - val_loss: 25.1920\n",
      "Epoch 252/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.8227 - val_loss: 25.5388\n",
      "Epoch 253/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.6171 - val_loss: 26.2218\n",
      "Epoch 254/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.7579 - val_loss: 25.4606\n",
      "Epoch 255/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.6156 - val_loss: 23.7338\n",
      "Epoch 256/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.1996 - val_loss: 26.0645\n",
      "Epoch 257/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.2956 - val_loss: 24.2228\n",
      "Epoch 258/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.4611 - val_loss: 22.8095\n",
      "Epoch 259/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.4769 - val_loss: 22.9917\n",
      "Epoch 260/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.5450 - val_loss: 25.4991\n",
      "Epoch 261/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.6668 - val_loss: 25.9405\n",
      "Epoch 262/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.8617 - val_loss: 24.5784\n",
      "Epoch 263/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.5018 - val_loss: 25.9823\n",
      "Epoch 264/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 16.8297 - val_loss: 23.9066\n",
      "Epoch 265/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.3372 - val_loss: 22.5516\n",
      "Epoch 266/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.1724 - val_loss: 24.6461\n",
      "Epoch 267/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.4008 - val_loss: 25.6791\n",
      "Epoch 268/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.7638 - val_loss: 25.4509\n",
      "Epoch 269/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.9879 - val_loss: 23.7808\n",
      "Epoch 270/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.6511 - val_loss: 28.6951\n",
      "Epoch 271/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 14.1164 - val_loss: 24.1701\n",
      "Epoch 272/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.0076 - val_loss: 23.0728\n",
      "Epoch 273/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 14.9977 - val_loss: 31.0342\n",
      "Epoch 274/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.4789 - val_loss: 30.7716\n",
      "Epoch 275/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.0061 - val_loss: 26.8119\n",
      "Epoch 276/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.5308 - val_loss: 24.9025\n",
      "Epoch 277/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.4209 - val_loss: 23.9809\n",
      "Epoch 278/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.7231 - val_loss: 28.0586\n",
      "Epoch 279/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.5072 - val_loss: 23.8356\n",
      "Epoch 280/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.4469 - val_loss: 22.1540\n",
      "Epoch 281/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.1106 - val_loss: 30.0485\n",
      "Epoch 282/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.8026 - val_loss: 22.3323\n",
      "Epoch 283/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.2124 - val_loss: 29.7320\n",
      "Epoch 284/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.6872 - val_loss: 23.5059\n",
      "Epoch 285/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 8.8391 - val_loss: 27.8500\n",
      "Epoch 286/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.1116 - val_loss: 24.7006\n",
      "Epoch 287/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.4906 - val_loss: 24.2917\n",
      "Epoch 288/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.4115 - val_loss: 26.4656\n",
      "Epoch 289/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.9402 - val_loss: 27.4103\n",
      "Epoch 290/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.5319 - val_loss: 22.2608\n",
      "Epoch 291/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.7804 - val_loss: 29.7560\n",
      "Epoch 292/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.0041 - val_loss: 26.4489\n",
      "Epoch 293/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.4480 - val_loss: 26.2551\n",
      "Epoch 294/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.7764 - val_loss: 23.2417\n",
      "Epoch 295/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.6777 - val_loss: 24.0362\n",
      "Epoch 296/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 8.8457 - val_loss: 22.2129\n",
      "Epoch 297/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.3196 - val_loss: 27.3412\n",
      "Epoch 298/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.0562 - val_loss: 30.7330\n",
      "Epoch 299/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.4428 - val_loss: 23.9976\n",
      "Epoch 300/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.5050 - val_loss: 23.0794\n",
      "Epoch 301/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.9414 - val_loss: 24.5089\n",
      "Epoch 302/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.6085 - val_loss: 23.3239\n",
      "Epoch 303/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.6428 - val_loss: 24.8707\n",
      "Epoch 304/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 8.4825 - val_loss: 27.0272\n",
      "Epoch 305/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.6665 - val_loss: 46.6034\n",
      "Epoch 306/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 15.6473 - val_loss: 26.1193\n",
      "Epoch 307/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.5321 - val_loss: 26.3487\n",
      "Epoch 308/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.0909 - val_loss: 24.1582\n",
      "Epoch 309/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.2292 - val_loss: 25.8766\n",
      "Epoch 310/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.2865 - val_loss: 23.8225\n",
      "Epoch 311/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.9988 - val_loss: 27.0526\n",
      "Epoch 312/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.1436 - val_loss: 24.8982\n",
      "Epoch 313/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.9205 - val_loss: 27.0180\n",
      "Epoch 314/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.1521 - val_loss: 24.2692\n",
      "Epoch 315/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.2952 - val_loss: 27.3722\n",
      "Epoch 316/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 8.1634 - val_loss: 22.9807\n",
      "Epoch 317/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 8.7830 - val_loss: 28.8941\n",
      "Epoch 318/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.2794 - val_loss: 26.2500\n",
      "Epoch 319/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.6885 - val_loss: 23.6961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.0622 - val_loss: 25.0102\n",
      "Epoch 321/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 13.8485 - val_loss: 26.2083\n",
      "Epoch 322/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.3186 - val_loss: 25.3946\n",
      "Epoch 323/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.0581 - val_loss: 25.7456\n",
      "Epoch 324/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.9812 - val_loss: 30.0581\n",
      "Epoch 325/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.2385 - val_loss: 26.5216\n",
      "Epoch 326/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 11.0249 - val_loss: 26.5545\n",
      "Epoch 327/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.8071 - val_loss: 27.2105\n",
      "Epoch 328/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.9388 - val_loss: 23.7200\n",
      "Epoch 329/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.6194 - val_loss: 23.1301\n",
      "Epoch 330/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4358 - val_loss: 22.7079\n",
      "Epoch 331/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.7348 - val_loss: 23.9219\n",
      "Epoch 332/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.4861 - val_loss: 25.1539\n",
      "Epoch 333/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.6017 - val_loss: 25.0947\n",
      "Epoch 334/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.5054 - val_loss: 25.3048\n",
      "Epoch 335/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.3496 - val_loss: 23.5935\n",
      "Epoch 336/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.8602 - val_loss: 24.0561\n",
      "Epoch 337/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.9581 - val_loss: 28.0077\n",
      "Epoch 338/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.5447 - val_loss: 28.3999\n",
      "Epoch 339/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.1106 - val_loss: 30.1430\n",
      "Epoch 340/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.2441 - val_loss: 25.5453\n",
      "Epoch 341/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.3296 - val_loss: 24.5285\n",
      "Epoch 342/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.8247 - val_loss: 24.3061\n",
      "Epoch 343/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.0014 - val_loss: 29.5090\n",
      "Epoch 344/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.2523 - val_loss: 26.2573\n",
      "Epoch 345/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.0695 - val_loss: 23.4233\n",
      "Epoch 346/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6282 - val_loss: 25.9150\n",
      "Epoch 347/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.0608 - val_loss: 29.3969\n",
      "Epoch 348/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.4784 - val_loss: 31.1910\n",
      "Epoch 349/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.4591 - val_loss: 23.8929\n",
      "Epoch 350/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5999 - val_loss: 23.4159\n",
      "Epoch 351/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.6552 - val_loss: 31.4192\n",
      "Epoch 352/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.4313 - val_loss: 29.3109\n",
      "Epoch 353/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.2467 - val_loss: 23.2973\n",
      "Epoch 354/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4532 - val_loss: 25.7751\n",
      "Epoch 355/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.0260 - val_loss: 24.5450\n",
      "Epoch 356/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.2706 - val_loss: 42.4294\n",
      "Epoch 357/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.4843 - val_loss: 27.3751\n",
      "Epoch 358/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.7504 - val_loss: 28.5606\n",
      "Epoch 359/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.6179 - val_loss: 24.4221\n",
      "Epoch 360/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.5977 - val_loss: 25.5375\n",
      "Epoch 361/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.9214 - val_loss: 32.1935\n",
      "Epoch 362/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.3549 - val_loss: 24.5837\n",
      "Epoch 363/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.4276 - val_loss: 27.8405\n",
      "Epoch 364/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.8664 - val_loss: 33.5667\n",
      "Epoch 365/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.6032 - val_loss: 24.3828\n",
      "Epoch 366/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5431 - val_loss: 28.0528\n",
      "Epoch 367/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.0619 - val_loss: 25.1208\n",
      "Epoch 368/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5667 - val_loss: 26.9101\n",
      "Epoch 369/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.3173 - val_loss: 25.6091\n",
      "Epoch 370/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.7734 - val_loss: 23.2107\n",
      "Epoch 371/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.4314 - val_loss: 29.9819\n",
      "Epoch 372/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.8171 - val_loss: 41.5533\n",
      "Epoch 373/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 13.3875 - val_loss: 27.2006\n",
      "Epoch 374/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 10.6032 - val_loss: 24.6545\n",
      "Epoch 375/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.6505 - val_loss: 29.7552\n",
      "Epoch 376/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.5054 - val_loss: 25.8126\n",
      "Epoch 377/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.2640 - val_loss: 23.7656\n",
      "Epoch 378/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.8789 - val_loss: 24.6344\n",
      "Epoch 379/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.3777 - val_loss: 24.8777\n",
      "Epoch 380/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 6.5060 - val_loss: 24.5960\n",
      "Epoch 381/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 8.9995 - val_loss: 25.6178\n",
      "Epoch 382/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.1190 - val_loss: 25.1902\n",
      "Epoch 383/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6998 - val_loss: 28.1939\n",
      "Epoch 384/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.5431 - val_loss: 25.8264\n",
      "Epoch 385/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.3415 - val_loss: 25.1349\n",
      "Epoch 386/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 14.4869 - val_loss: 23.3013\n",
      "Epoch 387/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 12.6892 - val_loss: 26.6973\n",
      "Epoch 388/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.5416 - val_loss: 25.0541\n",
      "Epoch 389/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.1974 - val_loss: 22.7112\n",
      "Epoch 390/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4803 - val_loss: 26.6835\n",
      "Epoch 391/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.4212 - val_loss: 29.1908\n",
      "Epoch 392/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 12.0377 - val_loss: 24.3640\n",
      "Epoch 393/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 9.1605 - val_loss: 26.7412\n",
      "Epoch 394/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.7169 - val_loss: 28.8634\n",
      "Epoch 395/400\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 9.4404 - val_loss: 23.2387\n",
      "Epoch 396/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.8706 - val_loss: 24.9279\n",
      "Epoch 397/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.0629 - val_loss: 25.5748\n",
      "Epoch 398/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 6.4681 - val_loss: 25.1118\n",
      "Epoch 399/400\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.4157 - val_loss: 26.8935\n",
      "Epoch 400/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/31 [..............................] - ETA: 0s - loss: 4.6870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "31/31 [==============================] - 0s 2ms/step - loss: 8.0227 - val_loss: 25.8397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d46a0a1130>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size = 10, epochs = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.935328 ],\n",
       "       [21.777075 ],\n",
       "       [26.694519 ],\n",
       "       [11.941988 ],\n",
       "       [19.96179  ],\n",
       "       [21.990126 ],\n",
       "       [23.018967 ],\n",
       "       [23.875505 ],\n",
       "       [17.653559 ],\n",
       "       [10.777466 ],\n",
       "       [ 7.297077 ],\n",
       "       [12.701988 ],\n",
       "       [16.200226 ],\n",
       "       [ 8.216388 ],\n",
       "       [54.981857 ],\n",
       "       [33.823986 ],\n",
       "       [22.755093 ],\n",
       "       [39.419186 ],\n",
       "       [38.213116 ],\n",
       "       [26.394041 ],\n",
       "       [26.195862 ],\n",
       "       [21.341759 ],\n",
       "       [19.940182 ],\n",
       "       [28.132277 ],\n",
       "       [22.289139 ],\n",
       "       [13.66794  ],\n",
       "       [18.707596 ],\n",
       "       [19.995241 ],\n",
       "       [37.80552  ],\n",
       "       [19.825813 ],\n",
       "       [15.246512 ],\n",
       "       [18.92429  ],\n",
       "       [20.540977 ],\n",
       "       [21.80546  ],\n",
       "       [25.23315  ],\n",
       "       [18.701069 ],\n",
       "       [ 9.420545 ],\n",
       "       [20.73342  ],\n",
       "       [13.914688 ],\n",
       "       [13.282741 ],\n",
       "       [25.049046 ],\n",
       "       [20.740868 ],\n",
       "       [19.932943 ],\n",
       "       [15.135781 ],\n",
       "       [25.307957 ],\n",
       "       [22.707314 ],\n",
       "       [21.044966 ],\n",
       "       [21.135122 ],\n",
       "       [15.385516 ],\n",
       "       [24.584063 ],\n",
       "       [ 9.598624 ],\n",
       "       [19.122334 ],\n",
       "       [21.808798 ],\n",
       "       [34.423035 ],\n",
       "       [19.45119  ],\n",
       "       [20.730062 ],\n",
       "       [21.185612 ],\n",
       "       [17.03045  ],\n",
       "       [ 9.674479 ],\n",
       "       [20.795876 ],\n",
       "       [25.730495 ],\n",
       "       [21.324993 ],\n",
       "       [34.346676 ],\n",
       "       [39.06859  ],\n",
       "       [20.051582 ],\n",
       "       [37.811016 ],\n",
       "       [16.431797 ],\n",
       "       [19.654493 ],\n",
       "       [14.59058  ],\n",
       "       [21.354904 ],\n",
       "       [18.977161 ],\n",
       "       [22.726593 ],\n",
       "       [33.469524 ],\n",
       "       [33.459305 ],\n",
       "       [23.25347  ],\n",
       "       [ 7.8011   ],\n",
       "       [37.02574  ],\n",
       "       [22.29038  ],\n",
       "       [25.172604 ],\n",
       "       [20.95713  ],\n",
       "       [25.523476 ],\n",
       "       [20.062155 ],\n",
       "       [17.379387 ],\n",
       "       [39.80831  ],\n",
       "       [40.69382  ],\n",
       "       [28.171171 ],\n",
       "       [22.267616 ],\n",
       "       [13.106987 ],\n",
       "       [34.790783 ],\n",
       "       [16.862612 ],\n",
       "       [22.177967 ],\n",
       "       [11.925663 ],\n",
       "       [24.635782 ],\n",
       "       [32.204346 ],\n",
       "       [20.747118 ],\n",
       "       [22.063385 ],\n",
       "       [ 5.2831903],\n",
       "       [26.7655   ],\n",
       "       [15.841982 ],\n",
       "       [20.062426 ],\n",
       "       [24.290955 ],\n",
       "       [24.634037 ],\n",
       "       [28.057415 ],\n",
       "       [22.38229  ],\n",
       "       [23.265045 ],\n",
       "       [21.985756 ],\n",
       "       [10.937556 ],\n",
       "       [17.762571 ],\n",
       "       [21.187632 ],\n",
       "       [25.763922 ],\n",
       "       [41.01179  ],\n",
       "       [11.920878 ],\n",
       "       [20.56699  ],\n",
       "       [14.788174 ],\n",
       "       [16.123632 ],\n",
       "       [21.598488 ],\n",
       "       [13.115406 ],\n",
       "       [22.179474 ],\n",
       "       [11.834309 ],\n",
       "       [52.184975 ],\n",
       "       [36.286    ],\n",
       "       [13.142666 ],\n",
       "       [20.955536 ],\n",
       "       [22.591507 ],\n",
       "       [22.35651  ],\n",
       "       [20.958088 ],\n",
       "       [35.32386  ],\n",
       "       [14.922859 ],\n",
       "       [21.893799 ],\n",
       "       [32.878803 ],\n",
       "       [16.295185 ],\n",
       "       [12.829666 ],\n",
       "       [15.150466 ],\n",
       "       [19.641834 ],\n",
       "       [12.626483 ],\n",
       "       [33.200832 ],\n",
       "       [22.40328  ],\n",
       "       [19.518665 ],\n",
       "       [25.387138 ],\n",
       "       [12.444971 ],\n",
       "       [11.283001 ],\n",
       "       [22.395767 ],\n",
       "       [41.70298  ],\n",
       "       [29.117868 ],\n",
       "       [30.991196 ],\n",
       "       [17.650738 ],\n",
       "       [35.757427 ],\n",
       "       [32.026173 ],\n",
       "       [12.436915 ],\n",
       "       [ 8.991194 ],\n",
       "       [31.203455 ],\n",
       "       [29.80468  ],\n",
       "       [11.75876  ],\n",
       "       [27.032267 ],\n",
       "       [18.17895  ],\n",
       "       [31.120346 ],\n",
       "       [20.97091  ],\n",
       "       [18.8461   ],\n",
       "       [20.14268  ],\n",
       "       [15.837947 ],\n",
       "       [12.71877  ],\n",
       "       [20.170338 ],\n",
       "       [40.483974 ],\n",
       "       [32.12579  ],\n",
       "       [22.37325  ],\n",
       "       [21.88752  ],\n",
       "       [24.69746  ],\n",
       "       [24.573627 ],\n",
       "       [22.58511  ],\n",
       "       [15.916887 ],\n",
       "       [31.649933 ],\n",
       "       [10.876921 ],\n",
       "       [19.084686 ],\n",
       "       [33.462852 ],\n",
       "       [22.772293 ],\n",
       "       [12.715213 ],\n",
       "       [19.99738  ],\n",
       "       [15.141076 ],\n",
       "       [21.610458 ],\n",
       "       [20.62318  ],\n",
       "       [16.666092 ],\n",
       "       [ 8.519195 ],\n",
       "       [20.671959 ],\n",
       "       [26.30624  ],\n",
       "       [16.906433 ],\n",
       "       [28.57386  ],\n",
       "       [22.212585 ],\n",
       "       [28.129566 ],\n",
       "       [42.78675  ],\n",
       "       [14.924658 ],\n",
       "       [12.705139 ],\n",
       "       [34.229713 ],\n",
       "       [33.40369  ],\n",
       "       [20.977917 ],\n",
       "       [49.762287 ],\n",
       "       [28.570335 ],\n",
       "       [25.928764 ],\n",
       "       [19.582096 ],\n",
       "       [29.97799  ],\n",
       "       [47.65864  ],\n",
       "       [25.930565 ],\n",
       "       [10.706826 ],\n",
       "       [40.65163  ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_prices = model.predict(x_test)\n",
    "predicted_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.6</td>\n",
       "      <td>24.935328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>21.777075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>26.694519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.3</td>\n",
       "      <td>11.941988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.2</td>\n",
       "      <td>19.961790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>24.3</td>\n",
       "      <td>22.063385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13.8</td>\n",
       "      <td>5.283190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>24.7</td>\n",
       "      <td>26.765499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14.1</td>\n",
       "      <td>15.841982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>18.7</td>\n",
       "      <td>20.062426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    real  predictions\n",
       "0   22.6    24.935328\n",
       "1   50.0    21.777075\n",
       "2   23.0    26.694519\n",
       "3    8.3    11.941988\n",
       "4   21.2    19.961790\n",
       "..   ...          ...\n",
       "95  24.3    22.063385\n",
       "96  13.8     5.283190\n",
       "97  24.7    26.765499\n",
       "98  14.1    15.841982\n",
       "99  18.7    20.062426\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_prices_array = np.array(y_test)\n",
    "predicted_prices_array = np.array(predicted_prices).flatten()\n",
    "df1 = pd.DataFrame({\"real\": real_prices_array, \"predictions\": predicted_prices_array})\n",
    "df1.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d46b7f58e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df1.plot.scatter(x=\"real\",y=0, color=\"DarkBlue\", label=\"Real\")\n",
    "df1.plot.scatter(x=0, y=\"predictions\", color=\"DarkGreen\", label=\"Predictions\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.839726574561727"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((predicted_prices.flatten()-y_test)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
